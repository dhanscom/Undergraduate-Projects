{"cells":[{"cell_type":"markdown","metadata":{"id":"SRsqmlcpJKPo"},"source":["Derick Hanscom\n","\n","Dr. Ryan White\n","\n","Homework #1"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1138,"status":"ok","timestamp":1645474359527,"user":{"displayName":"Derick Hanscom","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12422728024873374666"},"user_tz":300},"id":"389VVM6LJF82"},"outputs":[],"source":["# Library used to process data\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","# Library used to calculate performance metrics\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import r2_score\n","# Library used to perform Linear Regression\n","from sklearn.linear_model import LinearRegression\n","# Library used to import desired files\n","from google.colab import files"]},{"cell_type":"markdown","metadata":{"id":"bhUKZNwW5LlK"},"source":["##**Problem 1**\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":155,"status":"ok","timestamp":1645474360492,"user":{"displayName":"Derick Hanscom","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12422728024873374666"},"user_tz":300},"id":"4I8gG_8h1xoY"},"outputs":[],"source":["def normalize(columns):\n","  for i in columns:\n","    # Normalizes given columns according to the mean value in the column\n","    data[i] /= np.mean(data[i])"]},{"cell_type":"markdown","metadata":{"id":"IXjwGRzEKCeC"},"source":["Using get_dummies() in the Pandas library, Columns with features of Yes/No were processed as binary numbers in seperate columns. Having both of these columns decreased the accuracy of the model thus one was removed. Next was to manually process the two columns House Style and Subdivision and map the data to assigned numbers. Next was to turn the List Price data into an integer by removing all commas and dollar signs. The next step was to assign the data to the input and output variables uses for calculations. After the variables were assigned a 20/80 testing/training split was used to randomly seperate the data.\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7PpgjeFOKP2R","executionInfo":{"status":"ok","timestamp":1645474374561,"user_tz":300,"elapsed":193,"user":{"displayName":"Derick Hanscom","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12422728024873374666"}},"outputId":"01008985-5b18-410f-a0e9-3849efb9d9d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["(245, 20)\n"]}],"source":["# Allows user to upload file into google colab, Look under the text block to upload file\n","# File under inspection\n","data = pd.read_csv('Mount_Pleasant_Real_Estate_Data.csv')\n","\n","# Uses get_dummies to turn all the columns with Yes/No features into binary numbers in seperate columns\n","data = pd.get_dummies(data, columns=['Duplex?', 'New Owned?', 'Has Pool?', 'Has Dock?', 'Fenced Yard',\n","                                     'Screened Porch?', 'Golf Course?', 'Fireplace?'])\n","\n","# Drops all the added columns from get_dummies to improve accuracy, otherwise two columns with one being the inverse of the other\n","data = data.drop(columns=['Duplex?_Yes', 'New Owned?_Yes', 'Has Pool?_Yes', 'Has Dock?_Yes', 'Fenced Yard_Yes',\n","                          'Screened Porch?_Yes', 'Golf Course?_Yes', 'Fireplace?_Yes', 'Amenities', 'Misc Exterior',\n","                          'ID'])\n","\n","# Maps the House Style column to a numbered representation of for each style\n","data['House Style'] = data['House Style'].map({'Townhouse': 0, 'Traditional': 1, 'Cottage': 2, 'Patio': 3,\n","                                               'Craftsman': 4, 'Ranch': 5, 'Charleston Single': 6, 'Colonial': 7,\n","                                               'Contemporary': 8, \"Condominium\": 9, 'Condo Regime': 10})\n","\n","# Maps the Sub Division column to a numbered representation of for each location\n","data['Subdivision'] = data['Subdivision'].map({'Carolina Park': 0, 'Dunes West': 1, 'Park West': 2})\n","\n","# Drops any columns with NA values, prevents errors with type conversions\n","data.dropna(inplace=True)\n","\n","# Turns a price from a spreadsheet into an integer representation by removing dollar sign and comma\n","data['List Price'] = data['List Price'].str.replace('$', '', regex=True)\n","data['List Price'] = data['List Price'].str.replace(',', '', regex=True)\n","data['List Price'] = data['List Price'].astype(int)\n","\n","# List of columns that will need to be normalized\n","columns = ['List Price', 'Bedrooms', 'Baths - Total', 'Baths - Full', 'Baths - Half', 'Stories', 'Square Footage', 'Year Built', 'Acreage', 'Covered Parking Spots', 'Number of Fireplaces']\n","\n","# Calls the normalizing function to normalize colimns without Yes/No data types\n","normalize(columns)\n","\n","# Sets the List Price into a variable y, these will be our target outputs\n","y = data['List Price'].to_numpy()\n","\n","# Sets the remaining data excluding the list price as the X data\n","X = data.drop(columns=['List Price']).astype(float).to_numpy()\n","# Splits data into testing and training sets by a 20/80 percent split\n","(trainX, testX, trainY, testY) = train_test_split(X, y, test_size=0.2, random_state=1)"]},{"cell_type":"markdown","metadata":{"id":"jPmNFwt_Igbb"},"source":["## **Problem 2**"]},{"cell_type":"markdown","metadata":{"id":"Zpm6Al6wL6_Y"},"source":["The next step is to instantiate the Linear Regression class from sklearn. This class fits a linear line with the least error between the data and the line. The parameters that minimize this error are then applied to the testing data to create testing predictions. These predictions are then compared to the true values where we can determine how well the model predicts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFvC3jJbL7JE"},"outputs":[],"source":["# Initializes Sklearns Linear Regression Class\n","model = LinearRegression()\n","\n","# Uses the training data to determine theta parameters that minimize error\n","model.fit(trainX, trainY)\n","\n","# Uses the fitted model to predict the training outputs\n","trainPredictions = model.predict(trainX)\n","\n","# Uses the fitted model from the training data to predict the testing outputs\n","predictions = model.predict(testX)"]},{"cell_type":"markdown","metadata":{"id":"vlCL8sz1NEOs"},"source":["Next we need to check how well the training set did and then compare with the results of the testing set. This will be used to determine how well the parameters from the model fit the testing. If the error in each data set is relatively close then the model is successful and the model did not under or over fit the data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":129,"status":"ok","timestamp":1643584206522,"user":{"displayName":"Derick Hanscom","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12422728024873374666"},"user_tz":300},"id":"DOVuxQqKNEVg","outputId":"bb7dbb53-1651-452e-aa88-56cd5b89fe71"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Metrics\n","The r^2 score is 0.9150017079060816\n","The mean squared error on the training set is 0.020645511548928467\n","The mean absolute error on the training set is 0.10177031429334545\n","\n","\n","Testing Metrics\n","The r^2 score is 0.8818912275581665\n","The mean squared error on the test set is 0.026528940684087395\n","The mean absolute error on the test set is 0.10866616225110846\n"]}],"source":["print(\"Training Metrics\")\n","# print the coefficient of determination r^2 for the training set\n","print('The r^2 score is', r2_score(trainY, trainPredictions))\n","\n","# print quality metrics for training set\n","print('The mean squared error on the training set is', mean_squared_error(trainY, trainPredictions))\n","print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n","\n","print(f\"\\n\")\n","print(\"Testing Metrics\")\n","# print the coefficient of determination r^2 for the testing set\n","print('The r^2 score is', r2_score(testY, predictions))\n","\n","# print quality metrics for testing set\n","print('The mean squared error on the test set is', mean_squared_error(testY, predictions))\n","print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions))"]},{"cell_type":"markdown","source":["It seems that the hyper plane model fit quite well. Given how close the r^2 and the mean absolute error between the testing and training sets suggests that the model fit properly."],"metadata":{"id":"dgEjFVSxRCfg"}},{"cell_type":"markdown","metadata":{"id":"a3PZQyJmPRQy"},"source":["## **Problem 3**"]},{"cell_type":"markdown","source":["Problem three fits a polynomial to the data and then performs linear regression to determine the weights that will best fit the regression model to the data. By setting fit_intercept to false we acknwoldge that the constant term has already been accounted for. The last step is to calculate the metrics and perform a comparison of the two."],"metadata":{"id":"efENpr6tNdWo"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"7aAPUUkusVcI","colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"status":"error","timestamp":1645475857169,"user_tz":300,"elapsed":416,"user":{"displayName":"Derick Hanscom","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12422728024873374666"}},"outputId":"41974fa6-f15e-41e8-88fd-ccc6fd6ff203"},"outputs":[{"output_type":"error","ename":"AxisError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-acc169ac4ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Creates a polynomial for each feature for all data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mXh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;31m# arr, with the iteration axis at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 2"]}],"source":["# Takes in  largest desired degree of the polynomial\n","def univariatePolynomialBasis(m):\n","    def polynomialM(x):\n","        out = np.array([])\n","        for i in range(m + 1):\n","            # Adds a degree i term to the polynomial\n","            out = np.append(out, x ** i)\n","        return out\n","\n","    # Returns the polynomial function\n","    return polynomialM\n","\n","  # Calls the polynomial basis function and produces a degree 2 basis, quadratic\n","poly = univariatePolynomialBasis(2)\n","\n","# Creates a polynomial for each feature for all data\n","Xh = np.apply_along_axis(poly, 1, trainX)\n","model = LinearRegression(fit_intercept=False)\n","\n","print(Xh.shape)\n","print(trainX.shape)\n","\n","# Fits all of the polynomials from the training data to the prices of the training data \n","model.fit(Xh, trainY)\n","# Computes the training predictions using the theta values from fitted polynomial\n","trainPredictions = model.predict(Xh)\n","\n","# Generates the predicted prices using a polynomial from the testing data and the theta values generated from the initial fitting\n","predictions = model.predict(np.apply_along_axis(poly,1,testX))\n","\n","print(\"Training Metrics:\")\n","# print the coefficient of determination r^2 for the training set\n","print('The r^2 score for training is', r2_score(trainY, trainPredictions))\n","\n","# print quality metrics for the training set\n","print('The mean squared error on the training set is', mean_squared_error(trainY, trainPredictions))\n","print('The mean absolute error for training is', mean_absolute_error(trainY, trainPredictions))\n","\n","print(f\"\\n\")\n","print(\"Testing Metrics:\")\n","# print the coefficient of determination r^2 for the testing set\n","print('The r^2 score for testing is', r2_score(testY, predictions))\n","\n","# print quality metrics for the testing set\n","print('The mean squared error on the test set is', mean_squared_error(testY, predictions))\n","print('The mean absolute error for testing is', mean_absolute_error(testY, predictions))"]},{"cell_type":"markdown","source":["It seems that our model accurately predicts with the given data. The quadratic representation is slightly more accurate than the fitted hyperplane. The measure of likeness from the r^2 and the small amount of error in the MAE suggest that the model performed quite well. When comparing the r^2 and MAE between the testing and training sight it appears that our model was neither underfit nor overfit."],"metadata":{"id":"dRRIQRH8Qe02"}},{"cell_type":"markdown","metadata":{"id":"MX9ErYxGgIN0"},"source":["## **Problem #4**\n","\n","$a)$\n","\n"," Weighted Squared Sum Loss Function: Where P is a diagonal matrix with weights on the diagonal\n","\n","$L(\\theta)=\\sum_{i=1}^np_i(\\hat{f}(x_i)-y_i)^2=P(X\\theta-y)^T(X\\theta-y)=P(\\theta^TX^T-y^T)(X\\theta-y)=P(\\theta^TX^TX\\theta-\\theta^TX^Ty-y^TX\\theta+y^Ty)=P(\\theta^TX^TX\\theta-2\\theta^TX^Ty-y^Ty)$\n","\n","\n","$b)$\n","\n"," Next take derivative with repsect to theta and set to zero to solve for theta\n","\n","$\\frac{\\partial L(\\theta)}{\\partial \\theta}=(2X^TPX\\theta-2X^TPy)$\n","\n","$\\frac{\\partial L(\\theta)}{\\partial \\theta}=0 => \\hat{\\theta}=(X^TPX)^{-1}X^TPy$\n","\n","$c)$\n","\n","A penalizing scheme that increases over time will cause occurences close to the start time to be more impactful in the loss function than occurences with less significance farther away from the start time. This can be used in a stock model, it makes sense because the information of the previous day will be more pertinant to the prediction for the next day than the information from a year ago.\n","\n","**Problem 4 can also be solved by using the summation convention**\n","\n","$L(\\theta)=\\sum_{i=1}^np_i(\\hat{f}(x_i)-y_i)^2=\\sum_{i=1}^np_i(x_i\\theta_i-y_i)^2=\\sum_{i=1}^np_ix_i^2\\theta_i^2-2p_ix_i\\theta_iy_i+p_iy_i^2$\n","$\\frac{\\partial L(\\theta)}{\\partial \\theta}=\\sum_{i=1}^n2p_i(x_i^2\\theta_i-x_iy_i)$\n","\n","$\\frac{\\partial L(\\theta)}{\\partial \\theta}=0=>\\hat{\\theta}=\\sum_{i=1}^ny_i/x_i$\n","\n","##$Bonus$\n","\n","We can make the columns of data named \"Misc Exterior\" and \"Amenities\" more useful by simply counting the number of additions that the house has. If the house comes furnished and comes with a porch and shed, the house will be of higher value. By counting the number of additions in each column, a number can be generated that roughly represents the extra pros of the house which will correlate with higher prices. Another idea would be to search for key words that correlate to higher prices and count the number of times those key words are used for each house. These changes would numerically influence the predictive nature of the model and could show to be useful."]}],"metadata":{"colab":{"name":"Homework#1.ipynb","provenance":[],"mount_file_id":"1wkv6Opmc_TWG09q5alO6OgCEIIyQ1gQZ","authorship_tag":"ABX9TyPyfooRKG5S4echNCzJrV4k"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}